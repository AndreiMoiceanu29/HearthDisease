{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n0     1   39        4.0              0         0.0     0.0                0   \n1     0   46        2.0              0         0.0     0.0                0   \n2     1   48        1.0              1        20.0     0.0                0   \n3     0   61        3.0              1        30.0     0.0                0   \n4     0   46        3.0              1        23.0     0.0                0   \n\n   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n\n   TenYearCHD  \n0           0  \n1           0  \n2           0  \n3           1  \n4           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units=64,activation='relu', input_dim= 15))\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='sgd', metrics=['accuracy'])","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"TenYearCHD\"]\ny","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"0       0\n1       0\n2       0\n3       1\n4       0\n       ..\n4233    1\n4234    0\n4235    0\n4236    0\n4237    0\nName: TenYearCHD, Length: 4238, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = df[df.columns[:-1]]\n\nX","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n0        1   39        4.0              0         0.0     0.0   \n1        0   46        2.0              0         0.0     0.0   \n2        1   48        1.0              1        20.0     0.0   \n3        0   61        3.0              1        30.0     0.0   \n4        0   46        3.0              1        23.0     0.0   \n...    ...  ...        ...            ...         ...     ...   \n4233     1   50        1.0              1         1.0     0.0   \n4234     1   51        3.0              1        43.0     0.0   \n4235     0   48        2.0              1        20.0     NaN   \n4236     0   44        1.0              1        15.0     0.0   \n4237     0   52        2.0              0         0.0     0.0   \n\n      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n0                   0             0         0    195.0  106.0   70.0  26.97   \n1                   0             0         0    250.0  121.0   81.0  28.73   \n2                   0             0         0    245.0  127.5   80.0  25.34   \n3                   0             1         0    225.0  150.0   95.0  28.58   \n4                   0             0         0    285.0  130.0   84.0  23.10   \n...               ...           ...       ...      ...    ...    ...    ...   \n4233                0             1         0    313.0  179.0   92.0  25.97   \n4234                0             0         0    207.0  126.5   80.0  19.71   \n4235                0             0         0    248.0  131.0   72.0  22.00   \n4236                0             0         0    210.0  126.5   87.0  19.16   \n4237                0             0         0    269.0  133.5   83.0  21.47   \n\n      heartRate  glucose  \n0          80.0     77.0  \n1          95.0     76.0  \n2          75.0     70.0  \n3          65.0    103.0  \n4          85.0     85.0  \n...         ...      ...  \n4233       66.0     86.0  \n4234       65.0     68.0  \n4235       84.0     86.0  \n4236       86.0      NaN  \n4237       80.0    107.0  \n\n[4238 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>4233</td>\n      <td>1</td>\n      <td>50</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>313.0</td>\n      <td>179.0</td>\n      <td>92.0</td>\n      <td>25.97</td>\n      <td>66.0</td>\n      <td>86.0</td>\n    </tr>\n    <tr>\n      <td>4234</td>\n      <td>1</td>\n      <td>51</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>43.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>207.0</td>\n      <td>126.5</td>\n      <td>80.0</td>\n      <td>19.71</td>\n      <td>65.0</td>\n      <td>68.0</td>\n    </tr>\n    <tr>\n      <td>4235</td>\n      <td>0</td>\n      <td>48</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248.0</td>\n      <td>131.0</td>\n      <td>72.0</td>\n      <td>22.00</td>\n      <td>84.0</td>\n      <td>86.0</td>\n    </tr>\n    <tr>\n      <td>4236</td>\n      <td>0</td>\n      <td>44</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>210.0</td>\n      <td>126.5</td>\n      <td>87.0</td>\n      <td>19.16</td>\n      <td>86.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>4237</td>\n      <td>0</td>\n      <td>52</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>269.0</td>\n      <td>133.5</td>\n      <td>83.0</td>\n      <td>21.47</td>\n      <td>80.0</td>\n      <td>107.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4238 rows Ã— 15 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)","execution_count":26,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_values = imputer.fit_transform(X)\ntransformed_values","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"array([[  1.        ,  39.        ,   4.        , ...,  26.97      ,\n         80.        ,  77.        ],\n       [  0.        ,  46.        ,   2.        , ...,  28.73      ,\n         95.        ,  76.        ],\n       [  1.        ,  48.        ,   1.        , ...,  25.34      ,\n         75.        ,  70.        ],\n       ...,\n       [  0.        ,  48.        ,   2.        , ...,  22.        ,\n         84.        ,  86.        ],\n       [  0.        ,  44.        ,   1.        , ...,  19.16      ,\n         86.        ,  81.96675325],\n       [  0.        ,  52.        ,   2.        , ...,  21.47      ,\n         80.        , 107.        ]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,x_test,y_train,y_test = train_test_split(transformed_values,y, test_size = 0.25, random_state = 42, shuffle = True)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train, epochs = 150)","execution_count":53,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n3178/3178 [==============================] - 0s 67us/step - loss: 0.4128 - accuracy: 0.8474\nEpoch 2/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4129 - accuracy: 0.8446\nEpoch 3/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4141 - accuracy: 0.8439\nEpoch 4/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4100 - accuracy: 0.8452\nEpoch 5/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4114 - accuracy: 0.8449\nEpoch 6/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4129 - accuracy: 0.8464\nEpoch 7/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4120 - accuracy: 0.8458\nEpoch 8/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4126 - accuracy: 0.8464\nEpoch 9/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4133 - accuracy: 0.8471\nEpoch 10/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4131 - accuracy: 0.8452\nEpoch 11/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4127 - accuracy: 0.8471\nEpoch 12/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4123 - accuracy: 0.8442\nEpoch 13/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4125 - accuracy: 0.8449\nEpoch 14/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4132 - accuracy: 0.8430\nEpoch 15/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4139 - accuracy: 0.8452\nEpoch 16/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4108 - accuracy: 0.8477\nEpoch 17/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4153 - accuracy: 0.8436\nEpoch 18/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4108 - accuracy: 0.8449\nEpoch 19/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4115 - accuracy: 0.8458\nEpoch 20/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4121 - accuracy: 0.8461\nEpoch 21/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4123 - accuracy: 0.8455\nEpoch 22/150\n3178/3178 [==============================] - 0s 43us/step - loss: 0.4118 - accuracy: 0.8461\nEpoch 23/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4111 - accuracy: 0.8468\nEpoch 24/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4110 - accuracy: 0.8468\nEpoch 25/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4103 - accuracy: 0.8452\nEpoch 26/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4103 - accuracy: 0.8471\nEpoch 27/150\n3178/3178 [==============================] - 0s 51us/step - loss: 0.4099 - accuracy: 0.8455\nEpoch 28/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4110 - accuracy: 0.8468\nEpoch 29/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4054 - accuracy: 0.8461\nEpoch 30/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4130 - accuracy: 0.8452\nEpoch 31/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4113 - accuracy: 0.8483\nEpoch 32/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4087 - accuracy: 0.8480\nEpoch 33/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4127 - accuracy: 0.8455\nEpoch 34/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4124 - accuracy: 0.8446\nEpoch 35/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4079 - accuracy: 0.8436\nEpoch 36/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4085 - accuracy: 0.8461\nEpoch 37/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4169 - accuracy: 0.8449\nEpoch 38/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4106 - accuracy: 0.8433\nEpoch 39/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4179 - accuracy: 0.8449\nEpoch 40/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4124 - accuracy: 0.8477\nEpoch 41/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4128 - accuracy: 0.8474\nEpoch 42/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4086 - accuracy: 0.8461\nEpoch 43/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4101 - accuracy: 0.8499\nEpoch 44/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4134 - accuracy: 0.8458\nEpoch 45/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4143 - accuracy: 0.8458\nEpoch 46/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4161 - accuracy: 0.8436\nEpoch 47/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4109 - accuracy: 0.8468\nEpoch 48/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4114 - accuracy: 0.8480\nEpoch 49/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4094 - accuracy: 0.8468\nEpoch 50/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4126 - accuracy: 0.8471\nEpoch 51/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4081 - accuracy: 0.8468\nEpoch 52/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4087 - accuracy: 0.8446\nEpoch 53/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4129 - accuracy: 0.8430\nEpoch 54/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4095 - accuracy: 0.8436\nEpoch 55/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4098 - accuracy: 0.8439\nEpoch 56/150\n3178/3178 [==============================] - 0s 43us/step - loss: 0.4095 - accuracy: 0.8446\nEpoch 57/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4116 - accuracy: 0.8439\nEpoch 58/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4109 - accuracy: 0.8461\nEpoch 59/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4104 - accuracy: 0.8446\nEpoch 60/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4090 - accuracy: 0.8439\nEpoch 61/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4104 - accuracy: 0.8449\nEpoch 62/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4097 - accuracy: 0.8455\nEpoch 63/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4089 - accuracy: 0.8452\nEpoch 64/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4112 - accuracy: 0.8468\nEpoch 65/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4101 - accuracy: 0.8452\nEpoch 66/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4112 - accuracy: 0.8464\nEpoch 67/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4092 - accuracy: 0.8442\nEpoch 68/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4137 - accuracy: 0.8480\nEpoch 69/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4099 - accuracy: 0.8452\nEpoch 70/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4110 - accuracy: 0.8452\nEpoch 71/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4157 - accuracy: 0.8449\nEpoch 72/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4123 - accuracy: 0.8477\nEpoch 73/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4086 - accuracy: 0.8468\nEpoch 74/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4103 - accuracy: 0.8424\nEpoch 75/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4080 - accuracy: 0.8442\nEpoch 76/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4093 - accuracy: 0.8464\nEpoch 77/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4072 - accuracy: 0.8464\nEpoch 78/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4098 - accuracy: 0.8449\nEpoch 79/150\n","name":"stdout"},{"output_type":"stream","text":"3178/3178 [==============================] - 0s 45us/step - loss: 0.4093 - accuracy: 0.8455\nEpoch 80/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4094 - accuracy: 0.8464\nEpoch 81/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4098 - accuracy: 0.8468\nEpoch 82/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4080 - accuracy: 0.8458\nEpoch 83/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4100 - accuracy: 0.8496\nEpoch 84/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4080 - accuracy: 0.8468\nEpoch 85/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4087 - accuracy: 0.8442\nEpoch 86/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4099 - accuracy: 0.8480\nEpoch 87/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4058 - accuracy: 0.8455\nEpoch 88/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4110 - accuracy: 0.8464\nEpoch 89/150\n3178/3178 [==============================] - 0s 51us/step - loss: 0.4101 - accuracy: 0.8477\nEpoch 90/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4095 - accuracy: 0.8480\nEpoch 91/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4095 - accuracy: 0.8452\nEpoch 92/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4116 - accuracy: 0.8455\nEpoch 93/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4104 - accuracy: 0.8461\nEpoch 94/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4081 - accuracy: 0.8446\nEpoch 95/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4114 - accuracy: 0.8471\nEpoch 96/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4086 - accuracy: 0.8474\nEpoch 97/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4113 - accuracy: 0.8446\nEpoch 98/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4112 - accuracy: 0.8468\nEpoch 99/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4109 - accuracy: 0.8458\nEpoch 100/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4078 - accuracy: 0.8480\nEpoch 101/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4075 - accuracy: 0.8480\nEpoch 102/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4103 - accuracy: 0.8446\nEpoch 103/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4100 - accuracy: 0.8455\nEpoch 104/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4116 - accuracy: 0.8446\nEpoch 105/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4087 - accuracy: 0.8483\nEpoch 106/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4095 - accuracy: 0.8474\nEpoch 107/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4071 - accuracy: 0.8461\nEpoch 108/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4085 - accuracy: 0.8477\nEpoch 109/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4096 - accuracy: 0.8483\nEpoch 110/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4094 - accuracy: 0.8464\nEpoch 111/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4085 - accuracy: 0.8458\nEpoch 112/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4091 - accuracy: 0.8468\nEpoch 113/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4085 - accuracy: 0.8468\nEpoch 114/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4115 - accuracy: 0.8433\nEpoch 115/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4082 - accuracy: 0.8452\nEpoch 116/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4101 - accuracy: 0.8439\nEpoch 117/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4072 - accuracy: 0.8455\nEpoch 118/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4084 - accuracy: 0.8464\nEpoch 119/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4105 - accuracy: 0.8458\nEpoch 120/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4097 - accuracy: 0.8480\nEpoch 121/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4077 - accuracy: 0.8474\nEpoch 122/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4073 - accuracy: 0.8452\nEpoch 123/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4102 - accuracy: 0.8464\nEpoch 124/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4075 - accuracy: 0.8461\nEpoch 125/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4100 - accuracy: 0.8490\nEpoch 126/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4096 - accuracy: 0.8480\nEpoch 127/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4067 - accuracy: 0.8458\nEpoch 128/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4063 - accuracy: 0.8452\nEpoch 129/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4052 - accuracy: 0.8483\nEpoch 130/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4106 - accuracy: 0.8480\nEpoch 131/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4074 - accuracy: 0.8490\nEpoch 132/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4076 - accuracy: 0.8464\nEpoch 133/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4108 - accuracy: 0.8449\nEpoch 134/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4110 - accuracy: 0.8461\nEpoch 135/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4094 - accuracy: 0.8442\nEpoch 136/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4084 - accuracy: 0.8455\nEpoch 137/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4082 - accuracy: 0.8474\nEpoch 138/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4079 - accuracy: 0.8474\nEpoch 139/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4080 - accuracy: 0.8477\nEpoch 140/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4084 - accuracy: 0.8471\nEpoch 141/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4087 - accuracy: 0.8452\nEpoch 142/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4089 - accuracy: 0.8480\nEpoch 143/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4079 - accuracy: 0.8464\nEpoch 144/150\n3178/3178 [==============================] - 0s 48us/step - loss: 0.4075 - accuracy: 0.8452\nEpoch 145/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4099 - accuracy: 0.8452\nEpoch 146/150\n3178/3178 [==============================] - 0s 46us/step - loss: 0.4189 - accuracy: 0.8430\nEpoch 147/150\n3178/3178 [==============================] - 0s 45us/step - loss: 0.4077 - accuracy: 0.8458\nEpoch 148/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4102 - accuracy: 0.8455\nEpoch 149/150\n3178/3178 [==============================] - 0s 47us/step - loss: 0.4101 - accuracy: 0.8452\nEpoch 150/150\n3178/3178 [==============================] - 0s 44us/step - loss: 0.4107 - accuracy: 0.8468\n","name":"stdout"},{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7ff9cc63b198>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"_,acc = model.evaluate(x_test,y_test)\nprint(\"Acc:{}\",acc*100)","execution_count":57,"outputs":[{"output_type":"stream","text":"1060/1060 [==============================] - 0s 65us/step\nAcc:{} 85.75471639633179\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}